import streamlit as st
import requests
import pandas as pd
import json
import datetime
import numpy as np
from sklearn.ensemble import IsolationForest # ì´ìƒì¹˜ ê°ì§€
from sklearn.preprocessing import StandardScaler #í‘œì¤€í™”
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import openai
import re
import os
from dotenv import load_dotenv
load_dotenv()


# âœ… ìµœì‹  OpenAI GPT API í‚¤
openai.api_key = os.getenv("OPENAI_API_KEY")

# âœ… ë„¤ì´ë²„ ë¶€ë™ì‚° API í—¤ë”
headers = {
    "Accept-Encoding": "gzip",
    "Host": "new.land.naver.com",
    "Referer": "https://new.land.naver.com/complexes/102378?ms=37.5018495,127.0438028,16&a=APT&b=A1&e=RETAIL",
    "Sec-Fetch-Dest": "empty",
    "Sec-Fetch-Mode": "cors",
    "Sec-Fetch-Site": "same-origin",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "authorization": "Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IlJFQUxFU1RBVEUiLCJpYXQiOjE3MDExNTE5MzcsImV4cCI6MTcwMTE2MjczN30.dbdp5VH2gNMCxpfj_WtFkYhjM1CkC-t0deZUfgQU-fw"
}

@st.cache_data #ë°ì´í„°ë¥¼ í•œ ë²ˆë§Œ ê³„ì‚°í•˜ê³  ì €ì¥í•´ë‘ëŠ” ê¸°ëŠ¥
def get_regions(cortar_no="0000000000"):
    url = f"https://new.land.naver.com/api/regions/list?cortarNo={cortar_no}"
    r = requests.get(url, headers=headers)
    temp = json.loads(r.text)
    codes = [r['cortarNo'] for r in temp['regionList']]
    names = [r['cortarName'] for r in temp['regionList']]
    return dict(zip(names, codes))

@st.cache_data
def get_complexes(region_code, real_estate_type):
    url = f"https://new.land.naver.com/api/regions/complexes?cortarNo={region_code}&realEstateType={real_estate_type}"
    r = requests.get(url, headers=headers)
    try:
        temp = json.loads(r.text)
        if "complexList" in temp:
            return [(item['complexName'], item['complexNo']) for item in temp['complexList']]
    except:
        return []
    return []

def search_listings(complex_ids, real_estate_type, trade_type):
    results = []
    for complex_id in complex_ids:
        page = 1
        while True:
            url = f"https://new.land.naver.com/api/articles/complex/{complex_id}?realEstateType={real_estate_type}&tradeType={trade_type}&page={page}"
            r = requests.get(url, headers=headers)
            try:
                temp = json.loads(r.text)
                if 'articleList' in temp:
                    results.extend(temp['articleList'])
                if not temp.get('isMoreData'):
                    break
                page += 1
            except:
                break
    return pd.DataFrame(results)

# âœ… UI
st.title("ğŸ¢ ë¶€ë™ì‚° ë§¤ë¬¼ ê²€ìƒ‰ê¸° + AI ë¶„ì„ê¸°")
st.markdown("ì‹¤ì‹œê°„ ë§¤ë¬¼ ê²€ìƒ‰, ì´ìƒíƒì§€, AI ê°€ê²©ì˜ˆì¸¡, GPT ìš”ì•½ê¹Œì§€ í•œ ë²ˆì—!")

with st.sidebar:
    st.header("1ï¸âƒ£ ê²€ìƒ‰ ì¡°ê±´ ì„ íƒ")
    sido_dict = get_regions()
    sido_name = st.selectbox("ì‹œ/ë„ ì„ íƒ", list(sido_dict.keys()))
    sigungu_dict = get_regions(sido_dict[sido_name])
    sigungu_name = st.selectbox("ì‹œ/êµ°/êµ¬ ì„ íƒ", list(sigungu_dict.keys()))
    dong_dict = get_regions(sigungu_dict[sigungu_name])
    dong_name = st.selectbox("ì/ë©´/ë™ ì„ íƒ", list(dong_dict.keys()))

    real_estate_options = {
        "ì•„íŒŒíŠ¸": "APT", "ì¬ê±´ì¶•": "JGC", "ì˜¤í”¼ìŠ¤í…”": "OPST",
        "ì¬ê°œë°œ": "JGB", "ë¶„ì–‘ê¶Œ": "ABYG", "ë¶„ì–‘ ì˜ˆì •": "PRE"
    }
    selected_types = st.multiselect("ì£¼íƒ êµ¬ë¶„", list(real_estate_options.keys()), default=["ì•„íŒŒíŠ¸"])
    real_estate_type = ":".join([real_estate_options[i] for i in selected_types]) or "APT"

    trade_type_options = {"ë§¤ë§¤": "A1", "ì „ì„¸": "B1", "ì›”ì„¸": "B2"}
    selected_trade = st.multiselect("ê±°ë˜ ìœ í˜•", list(trade_type_options.keys()), default=["ë§¤ë§¤"])
    trade_type = ":".join([trade_type_options[i] for i in selected_trade]) or "A1"

    complex_list = get_complexes(dong_dict[dong_name], real_estate_type)
    selected_complex_names = st.multiselect("ë‹¨ì§€ ì„ íƒ (ì„ íƒ ì•ˆí•´ë„ ê°€ëŠ¥)", [x[0] for x in complex_list])
    selected_complex_ids = [x[1] for x in complex_list if x[0] in selected_complex_names]

if st.button("ğŸ” ë§¤ë¬¼ ê²€ìƒ‰"):
    if not selected_complex_ids:
        if complex_list:
            st.info("âš ï¸ ë‹¨ì§€ ë¯¸ì„ íƒ â†’ ì „ì²´ ë‹¨ì§€ ë§¤ë¬¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
            selected_complex_ids = [x[1] for x in complex_list]
        else:
            st.warning("âŒ í•´ë‹¹ ë™ì— ë‹¨ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
    if selected_complex_ids:
        df = search_listings(selected_complex_ids, real_estate_type, trade_type)
        if df.empty:
            st.info("ğŸ™ ê²€ìƒ‰ëœ ë§¤ë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            table = df[["articleName", "tradeTypeName", "dealOrWarrantPrc", "area1", "area2", "articleFeatureDesc"]]
            table = table.rename(columns={
                "articleName": "ë§¤ë¬¼ëª…", "tradeTypeName": "ê±°ë˜ìœ í˜•", "dealOrWarrantPrc": "ê°€ê²©(ë§Œì›)",
                "area1": "ë©´ì (ã¡)", "area2": "ê³µê¸‰ë©´ì ", "articleFeatureDesc": "ë§¤ë¬¼íŠ¹ì§•"
            })
            st.subheader("ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼")
            st.dataframe(table)

            #ì—‘ì…€ë¡œ ì €ì¥í•˜ëŠ” ì½”ë“œ 
            now = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            file_name = f"ë§¤ë¬¼_{sido_name}_{dong_name}_{now}.xlsx"
            df.to_excel(file_name, index=False)
            with open(file_name, "rb") as f:
                st.download_button("ğŸ“¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", data=f, file_name=file_name)

# âœ… ë¶„ì„ê¸°
st.markdown("---")
st.title("ğŸ“Š AI ë¶„ì„ê¸°")

uploaded = st.file_uploader("ğŸ“ ë§¤ë¬¼ ì—‘ì…€ ì—…ë¡œë“œ", type=["xlsx"])
if uploaded:
    df = pd.read_excel(uploaded)
    # st.write("ğŸ“Œ ì»¬ëŸ¼ëª… ì›ë³¸:", df.columns.tolist())  # ì–´ë–¤ ì»¬ëŸ¼ëª…ì´ ìˆëŠ”ì§€ í™•ì¸
    df.columns = df.columns.str.strip()
    #st.write("ğŸ“Œ ê³µë°± ì œê±° í›„:", df.columns.tolist())  # ê²°ê³¼ í™•ì¸

    #ëˆ„êµ¬ë‚˜ ì•Œì•„ë³¼ ìˆ˜ ìˆê²Œ í•œê¸€ë¡œ ë³€ê²½
    display_columns = {
    "articleNo": "ë§¤ë¬¼ë²ˆí˜¸",
    "articleName": "ë§¤ë¬¼ëª…",
    "articleStatus": "ìƒíƒœ",
    "realEstateTypeCode": "ë¶€ë™ì‚°ìœ í˜•ì½”ë“œ",
    "realEstateTypeName": "ë¶€ë™ì‚°ìœ í˜•ëª…",
    "articleRealEstateTypeCode": "ë§¤ë¬¼ìœ í˜•ì½”ë“œ",
    "articleRealEstateTypeName": "ë§¤ë¬¼ìœ í˜•ëª…",
    "tradeTypeCode": "ê±°ë˜ìœ í˜•ì½”ë“œ",
    "tradeTypeName": "ê±°ë˜ìœ í˜•ëª…",
    "floorInfo": "ì¸µì •ë³´",
    "dealOrWarrantPrc": "ê°€ê²©(ë§Œì›)",
    "areaName": "ì§€ì—­ëª…",
    "area1": "ë©´ì (ã¡)",
    "area2": "ê³µê¸‰ë©´ì (ã¡)",
    "direction": "ë°©í–¥",
    "articleConfirmYmd": "ë“±ë¡ì¼",
    "articleFeatureDesc": "íŠ¹ì§•",
    "tagList": "íƒœê·¸",
    "buildingName": "ê±´ë¬¼ëª…",
    "latitude": "ìœ„ë„",
    "longitude": "ê²½ë„",
    "realtorName": "ì¤‘ê°œì‚¬ëª…",
    "cpName": "ì¤‘ê°œì‚¬ë¬´ì†Œëª…",
    "detailAddress": "ìƒì„¸ì£¼ì†Œ",
    "representativeImgUrl": "ëŒ€í‘œì´ë¯¸ì§€URL"
    # í•„ìš”í•œ ì»¬ëŸ¼ì€ ì¶”ê°€ ê°€ëŠ¥
}
    
    # ë³´ì—¬ì¤„ ë•Œë§Œ rename
    st.dataframe(df[list(display_columns.keys())].rename(columns=display_columns))

    #í•œì¤„ ë‚´ìš©ìš”ì•½
    st.subheader("ğŸ“‹ ì—…ë¡œë“œëœ ë°ì´í„°")
    st.dataframe(
        df[list(display_columns.keys())]
        .head()
        .rename(columns=display_columns)
    )

    # ìˆ«ì ì •ì œ
    if 'dealOrWarrantPrc' in df.columns:
        df['price_num'] = df['dealOrWarrantPrc'].astype(str).str.replace(",", "").str.extract(r'(\d+)').astype(float)
    if 'area1' in df.columns:
        df['area1'] = df['area1'].astype(str).str.extract(r'([\d\.]+)').astype(float)

    if st.checkbox("âœ… ì´ìƒ ë§¤ë¬¼ íƒì§€ (Isolation Forest)"):
        if 'price_num' in df.columns and 'area1' in df.columns:
            # âŠ í‰ë‹¹ ê°€ê²© ê³„ì‚°
            df['price_per_m2'] = df['price_num'] / df['area1']

            # â‹ í‰ë‹¹ê°€ í‰ê·  ê³„ì‚°
            avg_price_per_m2 = df['price_per_m2'].mean()

            # âŒ í‰ë‹¹ê°€ ì°¨ì´ ë¹„ìœ¨ ê³„ì‚°
            df['í‰ê·  ëŒ€ë¹„ ì°¨ì´(%)'] = ((df['price_per_m2'] - avg_price_per_m2) / avg_price_per_m2 * 100).round(1)

            # â ì´ìƒì¹˜ íƒì§€
            clf = IsolationForest(contamination=0.1, random_state=42)
            df['anomaly'] = clf.fit_predict(df[['price_per_m2']])

            # â ì´ìƒ ìœ í˜• íŒë³„
            df['ì´ìƒ ìœ í˜•'] = df.apply(lambda row:
                "ê³ ê°€ ì´ìƒì¹˜" if row['anomaly'] == -1 and row['í‰ê·  ëŒ€ë¹„ ì°¨ì´(%)'] > 0 else (
                "ì €ê°€ ì´ìƒì¹˜" if row['anomaly'] == -1 and row['í‰ê·  ëŒ€ë¹„ ì°¨ì´(%)'] < 0 else "-"), axis=1)

            # âœ… ì „ì²´ ê²°ê³¼ êµ¬ì„±
            result = df[['articleName', 'dealOrWarrantPrc', 'area1', 'price_per_m2', 'í‰ê·  ëŒ€ë¹„ ì°¨ì´(%)', 'ì´ìƒ ìœ í˜•']].rename(columns={
                "articleName": "ë§¤ë¬¼ëª…",
                "dealOrWarrantPrc": "ê°€ê²©(ë§Œì›)",
                "area1": "ë©´ì (ã¡)",
                "price_per_m2": "í‰ë‹¹ê°€(ë§Œì›)"
            })

            # âœ… ì´ìƒì¹˜ë§Œ í•„í„°ë§
            abnormal_df = result[result['ì´ìƒ ìœ í˜•'] != "-"]

            if not abnormal_df.empty:
                st.subheader("âš ï¸ ì´ìƒ ë§¤ë¬¼ ë¦¬ìŠ¤íŠ¸ (í‰ë‹¹ê°€ ê¸°ì¤€)")
                st.dataframe(abnormal_df)
            else:
                st.info("âœ… ì´ìƒì¹˜ë¡œ ê°ì§€ëœ ë§¤ë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")

            # âœ… ì „ì²´ ê²°ê³¼ ì¶œë ¥
            st.subheader("ğŸ“‹ ì „ì²´ ë§¤ë¬¼ ê²°ê³¼")
            st.dataframe(result)


    if st.checkbox("âœ… AI ê°€ê²© ì˜ˆì¸¡ (ë©´ì â†’ê°€ê²©)"):
        if 'area1' in df.columns and 'price_num' in df.columns:
            x = df['area1'].values.reshape(-1, 1)
            y = df['price_num'].values

            # âœ… ê°„ë‹¨í•œ ì‹ ê²½ë§ ëª¨ë¸
            model = Sequential()
            model.add(Dense(32, input_dim=1, activation='relu'))
            model.add(Dense(16, activation='relu'))
            model.add(Dense(1))
            model.compile(loss='mse', optimizer=Adam(0.01))
            model.fit(x, y, epochs=30, verbose=0)

            # âœ… ì˜ˆì¸¡ ë° ì»¬ëŸ¼ ì¶”ê°€
            df['ì ì • ê°€ê²©(ë§Œì›)'] = model.predict(df['area1'].values.reshape(-1, 1)).astype(int)

            # âœ… ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬
            result = df[['articleName', 'area1', 'dealOrWarrantPrc', 'ì ì • ê°€ê²©(ë§Œì›)']].rename(columns={
                "articleName": "ë§¤ë¬¼ëª…",
                "area1": "ë©´ì (ã¡)",
                "dealOrWarrantPrc": "ì‹¤ì œ ê°€ê²©(ë§Œì›)"
            })

            st.success("âœ… AIê°€ ì˜ˆì¸¡í•œ ì ì • ê°€ê²©ì…ë‹ˆë‹¤. ì‹¤ì œ ê°€ê²©ê³¼ ë¹„êµí•´ë³´ì„¸ìš”!")
            st.dataframe(result)

        else:
            st.warning("âŒ ë©´ì  ë˜ëŠ” ê°€ê²© ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")


    # ğŸ“ í…ìŠ¤íŠ¸ AI ë¶„ì„ê¸°
    st.markdown("---")
    st.title("ğŸ“ í…ìŠ¤íŠ¸ AI ë¶„ì„ê¸°")

    # ğŸ” ì—­ë³€í™˜ìš©: í•œê¸€ â†’ ì˜ë¬¸
    reverse_display_columns = {v: k for k, v in display_columns.items()}

    # âœ… í…ìŠ¤íŠ¸ ì»¬ëŸ¼ í›„ë³´ (object íƒ€ì…)
    text_column_options = [col for col in df.columns if df[col].dtype == 'object']

    # âœ… ë³´ê¸°ìš© ì´ë¦„ ìƒì„±
    text_column_labels = [display_columns.get(col, col) for col in text_column_options]

    # âœ… ì‚¬ìš©ìì—ê²ŒëŠ” í•œê¸€ ì»¬ëŸ¼ ë³´ì—¬ì£¼ê³  ë‚´ë¶€ì—ì„  ì˜ë¬¸ ì»¬ëŸ¼ ì²˜ë¦¬
    selected_label = st.selectbox(
        "ë¶„ì„í•  í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì„ íƒ",
        text_column_labels,
        index=text_column_labels.index("íŠ¹ì§•") if "íŠ¹ì§•" in text_column_labels else 0
    )
    text_column = reverse_display_columns.get(selected_label, selected_label)

    # âœ… ì „ì²˜ë¦¬
    df['clean_text'] = df[text_column].astype(str) \
        .str.replace(r"[^\uAC00-\uD7A3a-zA-Z0-9\s]", "", regex=True) \
        .str.lower()

    # âœ… ìƒ˜í”Œ ì¶œë ¥
    st.write("âœ… ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ:")
    st.write(df['clean_text'].head(30).tolist())

    # âœ… âŠ ë¹ˆë„ìˆ˜ ê¸°ë°˜ ë‹¨ì–´ ë¶„ì„
    if st.checkbox("ğŸ“Œ âŠ ë¹ˆë„ìˆ˜ ê¸°ë°˜ ë‹¨ì–´ ë¶„ì„ (BoW)"):
        from sklearn.feature_extraction.text import CountVectorizer

        vectorizer = CountVectorizer(max_features=20)
        X = vectorizer.fit_transform(df['clean_text'])
        word_counts = X.toarray().sum(axis=0)
        words = vectorizer.get_feature_names_out()

        freq_df = pd.DataFrame({'ë‹¨ì–´': words, 'ë¹ˆë„ìˆ˜': word_counts})
        freq_df = freq_df.sort_values(by='ë¹ˆë„ìˆ˜', ascending=False)

        st.subheader("ğŸ“Š ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ë‹¨ì–´ Top 20")
        st.dataframe(freq_df)

        # ë‚˜ëˆ” ê³ ë”• ë¬¸ì œë¡œ ì „ì²´ ì£¼ì„í‘œì‹œ
        # fig, ax = plt.subplots()
        # ax.bar(freq_df['ë‹¨ì–´'], freq_df['ë¹ˆë„ìˆ˜'])
        # plt.xticks(rotation=45)
        # st.pyplot(fig)



    # âœ… â‹ TF-IDF ê¸°ë°˜ ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„
    if st.checkbox("ğŸ“Œ â‹ TF-IDF ê¸°ë°˜ ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„ (ìœ ì‚¬í•œ ë§¤ë¬¼ ì¶”ì²œ)"):
        
        # í•„ìš”í•œ ê¸°ë³¸ íŒ¨í‚¤ì§€
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity

        # TF-IDF ë²¡í„° ìƒì„±
        tfidf = TfidfVectorizer()
        tfidf_matrix = tfidf.fit_transform(df['clean_text'])

        # âœ… ë§¤ë¬¼ëª… + íŠ¹ì§• í‘œì‹œìš© ì¡°í•©
        df['combo_name'] = df['articleName'] + " | " + df[text_column].fillna("")

        # âœ… ì‚¬ìš©ìì—ê²Œ ê¸°ì¤€ ë§¤ë¬¼ ì„ íƒí•˜ê²Œ í•˜ê¸°
        selected_combo = st.selectbox("ğŸ“Œ ê¸°ì¤€ì´ ë  ë§¤ë¬¼ ì„ íƒ", df['combo_name'].tolist())
        doc_index = df[df['combo_name'] == selected_combo].index[0]

        # âœ… í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°
        text_sim = cosine_similarity(tfidf_matrix[doc_index], tfidf_matrix).flatten()

        # âœ… ê°€ê²© ì°¨ì´ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° (0~1 ì‚¬ì´ë¡œ ë³€í™˜)
        price_target = df.loc[doc_index, 'price_num']
        price_scores = 1 - abs(df['price_num'] - price_target) / price_target
        price_scores = price_scores.clip(lower=0)

        # âœ… í…ìŠ¤íŠ¸ + ê°€ê²© ìœ ì‚¬ë„ í‰ê·  ê³„ì‚°
        final_scores = (text_sim + price_scores) / 2

        # âœ… ìê¸° ìì‹  ì œì™¸ í›„ ìƒìœ„ 5ê°œ ë§¤ë¬¼ ì„ íƒ
        sim_scores = list(enumerate(final_scores))
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
        sim_scores = [s for s in sim_scores if s[0] != doc_index][:5]

        # âœ… ê²°ê³¼ ì¶œë ¥
        st.subheader("ğŸ“Œ ê¸°ì¤€ ë§¤ë¬¼")
        st.write(df.iloc[doc_index][['articleName', 'dealOrWarrantPrc', text_column]])

        st.subheader("ğŸ§© ìœ ì‚¬í•œ ë§¤ë¬¼ Top 5 (í…ìŠ¤íŠ¸ + ê°€ê²©)")
        for idx, score in sim_scores:
            price_diff_percent = round((df.loc[idx, 'price_num'] - price_target) / price_target * 100, 1)
            price_trend = "ğŸ“‰ ì €ë ´í•¨" if price_diff_percent < 0 else "ğŸ“ˆ ë¹„ìŒˆ"
            
            st.markdown(f"**ğŸ”¹ ìœ ì‚¬ë„ ì ìˆ˜: {score:.2f}** ({price_trend}, {abs(price_diff_percent)}%)")
            st.write(df.iloc[idx][['articleName', 'dealOrWarrantPrc', text_column]])


    
    # âœ… âŒ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„ë¥˜
    # âœ… âŒ AIê°€ ë§¤ë¬¼ ì„¤ëª…ì„ ì½ê³  ìë™ìœ¼ë¡œ íƒœê·¸ë¥¼ ë¶™ì—¬ìš”!
    if st.checkbox("ğŸ“Œ âŒ AIê°€ ë§¤ë¬¼ ì„¤ëª…ì„ ë³´ê³  ìë™ ë¶„ë¥˜í•´ìš”"):
        from sklearn.feature_extraction.text import CountVectorizer
        from sklearn.model_selection import train_test_split
        from sklearn.naive_bayes import MultinomialNB
        from sklearn.metrics import classification_report
        import pandas as pd
        import re

        st.info("âœï¸ ë§¤ë¬¼ ì„¤ëª…ì„ ì½ê³ , AIê°€ ìë™ìœ¼ë¡œ íƒœê·¸ë¥¼ ë¶™ì´ëŠ” ì‹¤ìŠµì´ì—ìš”!\n"
                "ì˜ˆ: 'í•œê°•ë·°', 'ì‹ ì¶•', 'ì—­ì„¸ê¶Œ', 'ë¡œì–„ì¸µ' ë“±")

        # âœ… ì¸µìˆ˜ì—ì„œ ìˆ«ìë§Œ ë½‘ì•„ì„œ 'floor' ë§Œë“¤ê¸°
        df['floor'] = df['floorInfo'].apply(
            lambda x: int(re.search(r'\d+', str(x)).group()) if re.search(r'\d+', str(x)) else -1
        )

        # âœ… ì„¤ëª…ì„ ë³´ê³  íƒœê·¸(label)ë¥¼ ìë™ìœ¼ë¡œ ë¶™ì´ëŠ” í•¨ìˆ˜
        def label_property(row):
            desc = str(row['articleFeatureDesc'])
            name = str(row['articleName'])
            floor = row['floor']

            if 'í•œê°•' in desc:
                return 'í•œê°•ë·°'
            elif 15 <= floor <= 20:
                return 'ë¡œì–„ì¸µ'
            elif 'ì—­' in desc or 'ì—­' in name:
                return 'ì—­ì„¸ê¶Œ'
            elif 'ì‹ ì¶•' in desc or 'ì‹ ì¶•ê¸‰' in desc:
                return 'ì‹ ì¶•'
            elif 'í’€ì˜µì…˜' in desc or 'ê°€ì „' in desc or 'ê°€êµ¬' in desc:
                return 'í’€ì˜µì…˜'
            elif 'ë°˜ë ¤' in desc or 'í«' in desc or 'ë™ë¬¼' in desc:
                return 'ë°˜ë ¤ë™ë¬¼'
            elif 'ì¦‰ì‹œì…ì£¼' in desc or 'ë°”ë¡œì…ì£¼' in desc:
                return 'ì¦‰ì‹œì…ì£¼'
            elif 'í™•ì¥' in desc:
                return 'í™•ì¥í˜•'
            elif 'ë‚¨í–¥' in desc:
                return 'ë‚¨í–¥'
            else:
                return 'ê¸°íƒ€'

        # âœ… íƒœê·¸ ë¶™ì´ê¸°
        df['label'] = df.apply(label_property, axis=1)

        # âœ… ë¬¸ì¥ì„ ìˆ«ìë¡œ ë°”ê¾¸ê¸°
        vectorizer = CountVectorizer()
        X = vectorizer.fit_transform(df['clean_text'])
        y = df['label']

        # âœ… ë°ì´í„° ë‚˜ëˆ„ê¸°
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # âœ… AI ë¶„ë¥˜ê¸° í›ˆë ¨
        clf = MultinomialNB()
        clf.fit(X_train, y_train)

        # âœ… ì˜ˆì¸¡ ì‹¤í–‰
        y_pred = clf.predict(X_test)

        # âœ… ë¶„ë¥˜ ê²°ê³¼ ìš”ì•½ (í‘œ í˜•íƒœë¡œ ë³´ê¸° ì‰½ê²Œ ë³€ê²½)
        st.subheader("ğŸ“Š AI ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½í‘œ (íƒœê·¸ë³„ ì •í™•ë„ ë¶„ì„)")

        report_dict = classification_report(y_test, y_pred, output_dict=True)
        report_df = pd.DataFrame(report_dict).transpose().reset_index()
        report_df = report_df.rename(columns={
            'index': 'íƒœê·¸',
            'precision': 'ì •í™•ë„ (Precision)',
            'recall': 'ì¬í˜„ìœ¨ (Recall)',
            'f1-score': 'F1 ì ìˆ˜',
            'support': 'ìƒ˜í”Œ ìˆ˜'
        })

        report_df[['ì •í™•ë„ (Precision)', 'ì¬í˜„ìœ¨ (Recall)', 'F1 ì ìˆ˜']] = report_df[
            ['ì •í™•ë„ (Precision)', 'ì¬í˜„ìœ¨ (Recall)', 'F1 ì ìˆ˜']
        ].round(2)

        visible_df = report_df[~report_df['íƒœê·¸'].isin(['accuracy', 'macro avg', 'weighted avg'])]

        st.dataframe(visible_df)

        st.info("""
    ğŸ“˜ ìš©ì–´ ì„¤ëª…:
    - **ì •í™•ë„ (Precision)**: AIê°€ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ì— ì§„ì§œ ë§ì€ ë¹„ìœ¨
    - **ì¬í˜„ìœ¨ (Recall)**: ì‹¤ì œ ì •ë‹µ ì¤‘ì—ì„œ AIê°€ ì˜ ë§ì¶˜ ë¹„ìœ¨
    - **F1 ì ìˆ˜**: ìœ„ ë‘˜ì„ ì¢…í•©í•œ ì ìˆ˜ (AIì˜ ì¢…í•© ì‹¤ë ¥ í‰ê°€)
    """)

        # âœ… ì˜ˆì¸¡ ë¼ë²¨ ë¶„í¬ ë³´ê¸°
        st.subheader("ğŸ“ˆ ì˜ˆì¸¡ëœ íƒœê·¸ë³„ ê±´ìˆ˜")
        label_counts_df = pd.DataFrame(pd.Series(y_pred).value_counts()).reset_index()
        label_counts_df.columns = ['ì˜ˆì¸¡ëœ íƒœê·¸', 'ê±´ìˆ˜']
        st.dataframe(label_counts_df)

        # âœ… ì‹¤ì œ vs ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ
        st.subheader("ğŸ” ì‹¤ì œ ì„¤ëª… vs AIê°€ ë¶™ì¸ íƒœê·¸ ë³´ê¸°")
        sample_df = pd.DataFrame({
            "ğŸ·ï¸ ë§¤ë¬¼ëª…": df.loc[y_test.index, 'articleName'].values,
            "ğŸ“ ì„¤ëª…": df.loc[y_test.index, 'clean_text'].values,
            "âœ… ì‹¤ì œ íƒœê·¸": y_test.values,
            "ğŸ¤– AI ì˜ˆì¸¡": y_pred
        }).reset_index(drop=True)

        st.dataframe(sample_df.head(10))



    
    # âœ… â Word2Vec ë‹¨ì–´ ì„ë² ë”© ì§ê´€ì  ì„¤ëª…í˜• í‘œë¡œ ë³´ê¸°
    if st.checkbox("ğŸ“Œ â Word2Vec ë‹¨ì–´ ì„ë² ë”© (ëœ»ê³¼ ìœ ì‚¬ ë‹¨ì–´ë¡œ ë³´ê¸°)"):
        from gensim.models import Word2Vec
        import pandas as pd

        st.info("ğŸ§  Word2Vecìœ¼ë¡œ í•™ìŠµëœ ë‹¨ì–´ì™€ ê·¸ ëœ», ë¹„ìŠ·í•œ ë‹¨ì–´ë¥¼ ì§ê´€ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.")

        # ë¬¸ì¥ ë‚˜ëˆ„ê¸°
        sentences = df['clean_text'].apply(lambda x: x.split()).tolist()

        # Word2Vec í›ˆë ¨
        model = Word2Vec(sentences, vector_size=50, window=5, min_count=2, workers=4, epochs=100)

        # ìƒìœ„ 20ê°œ ë‹¨ì–´ë§Œ ì‚¬ìš©
        vocab = list(model.wv.index_to_key)[:20]

        # ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ë‹¨ì–´ ì„¤ëª… ì‚¬ì „ (ì§ì ‘ ì±„ìš°ëŠ” ë¶€ë¶„, ì‹¤ì „ì´ë©´ GPTë¡œ ìë™ ìƒì„± ê°€ëŠ¥)
        word_explanations = {
            "í•œê°•": "ì„œìš¸ ì¤‘ì‹¬ì„ íë¥´ëŠ” ê°•. ì¡°ë§ì´ ì¢‹ì€ ì§‘ì—ì„œ ë³´ì´ëŠ” ê²½ìš°ê°€ ë§ìŒ",
            "ì•„íŒŒíŠ¸": "ì£¼ê±°ìš© ê±´ë¬¼. í•œêµ­ì—ì„œ ê°€ì¥ ì¼ë°˜ì ì¸ ì§‘ í˜•íƒœ",
            "ì—­ì„¸ê¶Œ": "ì§€í•˜ì² ì—­ ê°€ê¹Œìš´ ê³³. êµí†µì´ í¸ë¦¬í•¨",
            "ì €ì¸µ": "1~5ì¸µ ì •ë„ ë‚®ì€ ì¸µ",
            "ê³ ì¸µ": "15ì¸µ ì´ìƒ ë†’ì€ ì¸µ",
            "ì‹ ì¶•": "ì§€ì€ ì§€ ì–¼ë§ˆ ì•ˆ ëœ ìƒˆ ì•„íŒŒíŠ¸",
            "ë§¤ë§¤": "ë¶€ë™ì‚°ì„ ì‚¬ê³ íŒŒëŠ” ê±°ë˜ ë°©ì‹",
            "ì „ì„¸": "ë³´ì¦ê¸ˆë§Œ ë‚´ê³  ì¼ì • ê¸°ê°„ ì§‘ì— ì‚¬ëŠ” ë°©ì‹",
            "í…Œë¼ìŠ¤": "ì•¼ì™¸ ê³µê°„ì´ ë”¸ë¦° êµ¬ì¡°",
            "ë¦¬ëª¨ë¸ë§": "ì§‘ ì•ˆì„ ìƒˆë¡œ ê³ ì¹œ ìƒíƒœ",
            # ê·¸ ì™¸ëŠ” ìë™ ìƒì„±
        }

        # ê²°ê³¼ ì •ë¦¬
        rows = []
        for word in vocab:
            try:
                similar = model.wv.most_similar(word, topn=3)
                similar_words = ', '.join([w for w, _ in similar])
            except:
                similar_words = '-'
            rows.append({
                "ë‹¨ì–´": word,
                "ëœ» (ì„¤ëª…)": word_explanations.get(word, "â“ì´í•´ê°€ ì–´ë ¤ìš´ ë‹¨ì–´ ë˜ëŠ” ì„¤ëª… ì—†ìŒ"),
                "ë¹„ìŠ·í•œ ë‹¨ì–´ TOP3": similar_words
            })

        # í‘œ ì¶œë ¥
        result_df = pd.DataFrame(rows)
        st.dataframe(result_df)



    # âœ… í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë°”ê¾¸ê³ , ê¸¸ì´ë¥¼ ë”± 20ì¹¸ìœ¼ë¡œ ë§ì¶°ìš”
    if st.checkbox("ğŸ“Œ â í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ê³  ê¸¸ì´ë¥¼ ë§ì¶°ìš” (Tokenizer + Padding)"):
        from tensorflow.keras.preprocessing.text import Tokenizer
        from tensorflow.keras.preprocessing.sequence import pad_sequences

        st.info("âœï¸ AIëŠ” ê¸€ìë¥¼ ë°”ë¡œ ì´í•´í•˜ì§€ ëª»í•´ìš”.\n"
                "ê·¸ë˜ì„œ ë‹¨ì–´ë¥¼ ìˆ«ìë¡œ ë°”ê¾¸ê³ , ê¸¸ì´ë„ ë˜‘ê°™ì´ ë§ì¶°ì¤˜ì•¼ í•´ìš”!")

        # 1ï¸âƒ£ ë‹¨ì–´ë¥¼ ìˆ«ìë¡œ ë°”ê¾¸ëŠ” ì‚¬ì „ ë§Œë“¤ê¸°
        tokenizer = Tokenizer(num_words=1000, oov_token="<ê¸°íƒ€>")
        tokenizer.fit_on_texts(df['clean_text'])

        # 2ï¸âƒ£ ë¬¸ì¥ì„ ìˆ«ì ëª©ë¡ìœ¼ë¡œ ë°”ê¾¸ê¸°
        sequences = tokenizer.texts_to_sequences(df['clean_text'])

        # 3ï¸âƒ£ ìˆ«ì ëª©ë¡ì˜ ê¸¸ì´ë¥¼ ë”± 20ì¹¸ìœ¼ë¡œ ë§ì¶°ì£¼ê¸°
        padded = pad_sequences(sequences, maxlen=20, padding='post', truncating='post')

        # âœ… ë‹¨ì–´ ì‚¬ì „ ë³´ê¸° (ìƒìœ„ 10ê°œ)
        st.subheader("ğŸ”¤ ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ì´ ì–´ë–¤ ìˆ«ìë¡œ ë°”ë€Œì—ˆëŠ”ì§€ ë³¼ê¹Œìš”?")
        word_index = tokenizer.word_index
        st.write(dict(list(word_index.items())[:10]))

        # âœ… ì˜ˆì‹œ ë¬¸ì¥ ê³ ë¥´ê¸°
        st.subheader("ğŸ“˜ ì•„ë˜ì—ì„œ ë¬¸ì¥ì„ í•˜ë‚˜ ê³¨ë¼ë³´ì„¸ìš”!")
        sample_texts = df['clean_text'].dropna().unique().tolist()
        selected_text = st.selectbox("ğŸ‘‡ ë¬¸ì¥ì„ ê³ ë¥´ë©´ ìˆ«ìë¡œ ë°”ê¾¸ëŠ” ê³¼ì •ì„ ë³´ì—¬ë“œë ¤ìš”", sample_texts)

        # âœ… ì„ íƒí•œ ë¬¸ì¥ ìˆ«ìë¡œ ë°”ê¾¸ê¸°
        seq = tokenizer.texts_to_sequences([selected_text])[0]
        padded_seq = pad_sequences([seq], maxlen=20, padding='post', truncating='post')[0]

        st.write("ğŸ” ì„ íƒí•œ ë¬¸ì¥:", selected_text)
        
        # âœ… ë³´ê¸° ì‰½ê²Œ ë‹¨ì–´-ìˆ«ì ë§¤í•‘ í‘œë¡œ ë³´ì—¬ì£¼ê¸°
        tokens = selected_text.split()
        st.subheader("ğŸ”¢ ë‹¨ì–´ê°€ ì–´ë–¤ ìˆ«ìë¡œ ë°”ë€Œì—ˆëŠ”ì§€ ë³¼ê¹Œìš”?")
        display_rows = []

        for i in range(20):
            if i < len(seq):
                display_rows.append({
                    "ìœ„ì¹˜": i + 1,
                    "ë‹¨ì–´": tokens[i] if i < len(tokens) else "(ë‹¨ì–´ ì—†ìŒ)",
                    "ìˆ«ì ID": seq[i],
                    "ì„¤ëª…": "ë‹¨ì–´ë¥¼ ìˆ«ìë¡œ ë°”ê¾¼ ê°’"
                })
            else:
                display_rows.append({
                    "ìœ„ì¹˜": i + 1,
                    "ë‹¨ì–´": "(ë¹ˆì¹¸)",
                    "ìˆ«ì ID": 0,
                    "ì„¤ëª…": "0ìœ¼ë¡œ ì±„ì›Œì§„ ë¹ˆì¹¸"
                })

        st.dataframe(pd.DataFrame(display_rows))

        # âœ… ì „ì²´ ê²°ê³¼ ìš”ì•½
        st.success(f"""ğŸ§¾ ì „ì²´ ì •ë¦¬:
    ì´ {padded.shape[0]}ê°œì˜ ë¬¸ì¥ì´ ìˆê³ ,  
    ëª¨ë“  ë¬¸ì¥ì„ ìˆ«ì 20ê°œì§œë¦¬ë¡œ í†µì¼í–ˆì–´ìš”!  
    ì§§ì€ ë¬¸ì¥ì€ ë’¤ì— 0ìœ¼ë¡œ ì±„ìš°ê³ ,  
    ê¸´ ë¬¸ì¥ì€ ë’¤ì—ì„œ ì˜ëì–´ìš”.  
    â†’ ì´ì œ AIê°€ ì½ê³  ê³„ì‚°í•˜ê¸° í¸í•´ì¡Œì–´ìš” ğŸ˜Š""")


    # âœ… â ìƒì—…ì  ê°ì„± ë¶„ì„ (ê¸°ì¤€ ì™„í™” ë‹¤ì¤‘ì„ íƒ + ìš°ì„ ìˆœìœ„ ì ìˆ˜)
    if st.checkbox("ğŸ“Œ â ìƒì—…ì  ê°ì„± ë¶„ì„ (LSTM + ê¸°ì¤€ ì™„í™” + ìš°ì„ ìˆœìœ„)"):

        from tensorflow.keras.preprocessing.text import Tokenizer
        from tensorflow.keras.preprocessing.sequence import pad_sequences
        from tensorflow.keras.utils import to_categorical
        from tensorflow.keras.models import Sequential
        from tensorflow.keras.layers import Embedding, LSTM, Dense
        from sklearn.model_selection import train_test_split
        from sklearn.preprocessing import LabelEncoder
        import numpy as np
        import re

        st.info("ğŸ  ì „ì²´ ë§¤ë¬¼ ì„¤ëª…ì„ AIê°€ ì½ê³ , ê°ì •ì„ ì˜ˆì¸¡í•˜ê³ , ì‹¤ê±°ë˜ ê¸°ì¤€ìœ¼ë¡œ ìš°ì„  ì¶”ì²œí•´ì¤˜ìš”!")

        # âœ… ê°ì • ë¼ë²¨ ë¶„ë¥˜
        def label_rule(text):
            text = str(text)
            if any(x in text for x in ["ê¸‰ë§¤", "ì‹œì„¸ ì´í•˜", "ê°€ê²© ì°©í•¨", "ì¦‰ì‹œ ì…ì£¼", "ê¸‰í•˜ê²Œ íŒ”ì•„ìš”", "í• ì¸", "ë¹ ë¥¸ ë§¤ë„"]):
                return "ì €ë ´í•œ ë§¤ë¬¼"
            elif any(x in text for x in ["ë¹„ìŒˆ", "ì‹œì„¸ë³´ë‹¤ ë†’ìŒ", "ê³ í‰ê°€", "ë§¤ìš° ë¹„ìŒˆ", "í˜¸ê°€ ë†’ìŒ"]):
                return "ê³ í‰ê°€ ë§¤ë¬¼"
            elif any(x in text for x in ["ì—­ì„¸ê¶Œ", "ì‹ ì¶•", "ì „ë§ ì¢‹ìŒ", "ì¬ê±´ì¶• ê¸°ëŒ€", "í•™êµ°", "ê°•ë‚¨", "ì „ì„¸ê°€ìœ¨ ë†’ìŒ"]):
                return "íˆ¬ì ì¶”ì²œ"
            elif any(x in text for x in ["ì†ŒìŒ", "ì˜¤ë˜ë¨", "í•˜ì", "ìœ ì°°", "ë²Œë ˆ", "ì•ˆì¢‹ìŒ"]):
                return "íˆ¬ì ë§Œë¥˜"
            else:
                return "ë§¤ë ¥ì ì´ì§€ ì•ŠìŒ"

        # âœ… ì¶”ì²œì‚¬ìœ  ì„¤ëª…
        def recommend_reason(text):
            text = str(text)
            if "ê¸‰ë§¤" in text:
                return "ğŸ’¸ ê¸‰ë§¤ í‚¤ì›Œë“œ í¬í•¨"
            elif "ì—­ì„¸ê¶Œ" in text:
                return "ğŸš‰ ì—­ì„¸ê¶Œ ì…ì§€"
            elif "ì‹ ì¶•" in text:
                return "ğŸ—ï¸ ì‹ ì¶• ê±´ë¬¼"
            elif "ì „ë§ ì¢‹ìŒ" in text:
                return "ğŸŒ… ì „ë§ ìš°ìˆ˜"
            elif "ì¬ê±´ì¶• ê¸°ëŒ€" in text:
                return "ğŸ¢ ì¬ê±´ì¶• ê¸°ëŒ€"
            elif "ê°€ê²© ì°©í•¨" in text or "ì‹œì„¸ ì´í•˜" in text:
                return "ğŸ’° ì €ë ´í•œ ê°€ê²©"
            else:
                return "ğŸ” ì¼ë°˜ ë§¤ë¬¼"

        # âœ… í‰ë‹¹ê°€ê²© ê³„ì‚°
        def calculate_price_per_area(row):
            try:
                price = int(re.sub(r'\D', '', str(row.get('dealOrWarrantPrc', '0'))))
                area = float(row.get('area1', 0))
                return price / area if area > 0 else None
            except:
                return None

        df['sentiment_label'] = df['clean_text'].apply(label_rule)
        df['í‰ë‹¹ê°€ê²©'] = df.apply(calculate_price_per_area, axis=1)
        í‰ê· í‰ë‹¹ê°€ = df['í‰ë‹¹ê°€ê²©'].dropna().mean()

        # âœ… ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚°
        def smart_priority_score(row):
            score = 0
            text = str(row['clean_text'])

            if row['AI_ì˜ˆì¸¡ê°ì •'] in ['ì €ë ´í•œ ë§¤ë¬¼', 'íˆ¬ì ì¶”ì²œ']:
                score += 2
            if 'ì•„íŒŒíŠ¸' in row.get('articleName', ''):
                score += 2
            try:
                price_per_area = row['í‰ë‹¹ê°€ê²©']
                if price_per_area and price_per_area < í‰ê· í‰ë‹¹ê°€ * 0.9:
                    score += 3
            except:
                pass
            if "ê¸‰ë§¤" in text or "í• ì¸" in text:
                score += 2
            if "ì—­ì„¸ê¶Œ" in text:
                score += 1
            if "ì‹ ì¶•" in text:
                score += 1
            if "ì „ë§ ì¢‹ìŒ" in text or "ì¬ê±´ì¶• ê¸°ëŒ€" in text:
                score += 1
            if "í•™êµ°" in text or "ì „ì„¸ê°€ìœ¨" in text:
                score += 1
            if "ë¹„ìŒˆ" in text:
                score -= 1
            if "ì†ŒìŒ" in text or "í•˜ì" in text:
                score -= 1

            return score

        # âœ… í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        tokenizer = Tokenizer(num_words=1000, oov_token="<OOV>")
        tokenizer.fit_on_texts(df['clean_text'])
        sequences = tokenizer.texts_to_sequences(df['clean_text'])
        padded = pad_sequences(sequences, maxlen=20, padding='post')

        # âœ… ë¼ë²¨ ì¸ì½”ë”©
        label_encoder = LabelEncoder()
        labels_encoded = label_encoder.fit_transform(df['sentiment_label'])
        labels_categorical = to_categorical(labels_encoded, num_classes=5)

        # âœ… í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬
        X_train, X_test, y_train, y_test = train_test_split(padded, labels_categorical, test_size=0.2, random_state=42)

        # âœ… ëª¨ë¸ í•™ìŠµ
        model = Sequential()
        model.add(Embedding(input_dim=1000, output_dim=32, input_length=20))
        model.add(LSTM(64))
        model.add(Dense(5, activation='softmax'))
        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
        model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), verbose=0)

        # âœ… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        acc = model.evaluate(X_test, y_test, verbose=0)[1]
        st.success(f"ğŸ¯ ê°ì„± ë¶„ì„ ëª¨ë¸ ì •í™•ë„: {acc * 100:.2f}%")

        all_pred_probs = model.predict(padded)
        all_pred_labels = label_encoder.inverse_transform(np.argmax(all_pred_probs, axis=1))
        df['AI_ì˜ˆì¸¡ê°ì •'] = all_pred_labels
        df['ì¶”ì²œì‚¬ìœ '] = df.apply(lambda row: recommend_reason(row['clean_text']) if row['AI_ì˜ˆì¸¡ê°ì •'] in ['íˆ¬ì ì¶”ì²œ', 'ì €ë ´í•œ ë§¤ë¬¼'] else '', axis=1)
        df['ìš°ì„ ì¶”ì²œì ìˆ˜'] = df.apply(smart_priority_score, axis=1)

        # âœ… ì‚¬ìš©ì ì„ íƒ í•„í„°
        selected_labels = st.multiselect(
            "ğŸ”§ ì¶”ì²œ ë§¤ë¬¼ì— í¬í•¨í•  ê°ì • ì„ íƒ",
            options=['íˆ¬ì ì¶”ì²œ', 'ì €ë ´í•œ ë§¤ë¬¼', 'ê³ í‰ê°€ ë§¤ë¬¼', 'íˆ¬ì ë§Œë¥˜', 'ë§¤ë ¥ì ì´ì§€ ì•ŠìŒ'],
            default=['íˆ¬ì ì¶”ì²œ']
        )

        # âœ… í•„í„°ë§ í›„ highlight_df ìƒì„±
        highlight_df = df[df['AI_ì˜ˆì¸¡ê°ì •'].isin(selected_labels)]
        highlight_df = highlight_df.sort_values(by='ìš°ì„ ì¶”ì²œì ìˆ˜', ascending=False)

        # âœ… ê²°ê³¼ ì¶œë ¥
        st.subheader("ğŸ“‹ ì „ì²´ ë§¤ë¬¼ ê°ì • ë¶„ì„ ê²°ê³¼ (ì ìˆ˜ í¬í•¨)")
        st.dataframe(highlight_df[['articleName', 'clean_text', 'AI_ì˜ˆì¸¡ê°ì •', 'ì¶”ì²œì‚¬ìœ ', 'ìš°ì„ ì¶”ì²œì ìˆ˜']].head(5))

        if not highlight_df.empty:
            st.subheader("ğŸ’¡ AIê°€ ì¶”ì²œí•œ ìƒìœ„ ë§¤ë¬¼ (ìš°ì„ ìˆœìœ„ ë†’ì€ ìˆœ)")

            # âœ… í‘œì‹œí•˜ê³  ì‹¶ì€ ì»¬ëŸ¼ ëª©ë¡
            desired_cols = [
                'articleName', 'clean_text', 'dealOrWarrantPrc', 'realtorName',
                'í‰ë‹¹ê°€ê²©', 'AI_ì˜ˆì¸¡ê°ì •', 'ì¶”ì²œì‚¬ìœ ', 'ìš°ì„ ì¶”ì²œì ìˆ˜'
            ]

            # âœ… ì‹¤ì œ highlight_dfì— ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§
            existing_cols = [col for col in desired_cols if col in highlight_df.columns]

            # âœ… ìˆëŠ” ì»¬ëŸ¼ë§Œ ì¶œë ¥
            st.dataframe(highlight_df[existing_cols].head(5))

        else:
            st.warning("ğŸ“­ ì¡°ê±´ì— ë§ëŠ” ì¶”ì²œ ë§¤ë¬¼ì´ ì—†ìŠµë‹ˆë‹¤. ê°ì • í•„í„°ë¥¼ ë” ì™„í™”í•´ë³´ì„¸ìš”.")


    # âœ… GPT íˆ¬ì ë¦¬í¬íŠ¸ (AI ì¶”ì²œ ìƒìœ„ ë§¤ë¬¼ ê¸°ë°˜)
    st.markdown("---")
    st.title("ğŸ“ˆ íˆ¬ì ë¦¬í¬íŠ¸ (GPT ìƒì„±)")

    if st.button("ğŸ“Š AI ì¶”ì²œ ìƒìœ„ ë§¤ë¬¼ ê¸°ë°˜ íˆ¬ì ë¦¬í¬íŠ¸ ìƒì„±"):
        try:
            # ğŸ‘‰ ìƒìœ„ ì¶”ì²œ ë§¤ë¬¼ë§Œ í…ìŠ¤íŠ¸ë¡œ êµ¬ì„± (highlight_df ê¸°ì¤€)
            top_recommend = highlight_df.head(5)
            if top_recommend.empty:
                st.warning("ì¶”ì²œëœ ìƒìœ„ ë§¤ë¬¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ê°ì„± ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            else:
                text_summary = top_recommend[['articleName', 'dealOrWarrantPrc', 'area1']].to_string(index=False)
                sentiment_summary = top_recommend[['articleName', 'AI_ì˜ˆì¸¡ê°ì •', 'ì¶”ì²œì‚¬ìœ ']].to_string(index=False)

                # ğŸ“Œ í”„ë¡¬í”„íŠ¸ (ì´ˆë³´ íˆ¬ìì ê´€ì , ìƒìœ„ ì¶”ì²œ ë§¤ë¬¼ ê¸°ì¤€)
                prompt = f"""
                ë‹¹ì‹ ì€ ë¶€ë™ì‚° íˆ¬ì ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ëŠ” AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
                ì•„ë˜ AIê°€ ì¶”ì²œí•œ ë¶€ë™ì‚° ìƒìœ„ ë§¤ë¬¼ 5ê±´ì„ ë³´ê³ , ì´ˆë³´ íˆ¬ììë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë¶„ì„í•´ ì£¼ì„¸ìš”.

                ë‹¤ìŒ ë‚´ìš©ì„ ê¼­ í¬í•¨í•´ ì£¼ì„¸ìš”:
                1. ê° ë§¤ë¬¼ì˜ íˆ¬ì ë§¤ë ¥ë„(ì…ì§€, ê°€ê²© ë©”ë¦¬íŠ¸, ì¬ê±´ì¶• ê°€ëŠ¥ì„± ë“±)
                2. ìœ ì˜í•´ì•¼ í•  ìš”ì†Œ(ì†ŒìŒ, í•˜ì, ê³ í‰ê°€ ìš°ë ¤ ë“±)
                3. ì „ì²´ ë§¤ë¬¼ ì¤‘ ê°€ì¥ ì¶”ì²œí•  ë§Œí•œ ìˆœìœ„ ë° ì´ìœ 
                4. ì´ˆë³´ìê°€ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ìš©ì–´ë¥¼ ìµœëŒ€í•œ ì‰½ê²Œ ì„¤ëª…
                5. ë§ˆì§€ë§‰ì— 5ì¤„ ì´ë‚´ ë°œí‘œìš© ìš”ì•½ ë¬¸ë‹¨ í¬í•¨

                ğŸ”¸ ì¶”ì²œ ë§¤ë¬¼ ìš”ì•½ (ì´ë¦„ / ê°€ê²© / ë©´ì ):
                {text_summary}

                ğŸ”¸ ê°ì • ì˜ˆì¸¡ ë° ì¶”ì²œ ì‚¬ìœ :
                {sentiment_summary}

                ì´ì œ ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ íˆ¬ì ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
                """

                # âœ… GPT-3.5 Turbo í˜¸ì¶œ
                response = openai.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.6,
                    max_tokens=4000
                )

                gpt_report = response.choices[0].message.content.strip()

                st.subheader("ğŸ“‘ GPT íˆ¬ì ë¦¬í¬íŠ¸")
                st.markdown(gpt_report)

        except Exception as e:
            st.warning("âŒ GPT ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            st.error(e)


else:
    st.info("â¬†ï¸ ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¶„ì„ì´ ì‹œì‘ë©ë‹ˆë‹¤.")


